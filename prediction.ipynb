{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Model imports to test\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from DataPreprocessing import DataPreprocesser\n",
    "from MyDataset import MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect('dota2.db')\n",
    "cursor = connection.cursor()\n",
    "MyProcesser = DataPreprocesser(connection, cursor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataPreprocesser.to_dataframes(MyProcesser)  # Load the data into dataframes\n",
    "\n",
    "data = MyProcesser.merge_data()\n",
    "\n",
    "y = data.loc[:, 'radiant_win']\n",
    "X = data.drop(columns=['radiant_win'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp, counts = np.unique(y, return_counts=True)\n",
    "percentages = (counts / y.size) * 100\n",
    "\n",
    "bars = plt.bar([0,1], percentages, color=['skyblue', 'salmon'])\n",
    "\n",
    "for bar, percentage in zip(bars, percentages):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 1, f'{percentage:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "# Assign title and labels\n",
    "plt.title('Percent Distribution of Radiant vs Dire Wins')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Percentage (%)')\n",
    "bars[0].set_label('Dire Won')\n",
    "bars[1].set_label('Radiant Won')\n",
    "\n",
    "plt.legend()\n",
    "plt.xticks([0,1])  # 0 is seen as a dire win, 1 is seen as a radiant win\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(X_train, y_train, transforms=None)\n",
    "test_data = MyDataset(X_test, y_test, transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll be testing various models to use, and employing the best one\n",
    "\n",
    "# For now, no parameter modifications outside of default\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(),\n",
    "    \"Naive Bayes\": GaussianNB()\n",
    "}\n",
    "\n",
    "# I want to add some neural networks to be used\n",
    "neural_networks = {\n",
    "    \"MLP\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Models & Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Does the training itself\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics below\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # ROC_AUC\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    print(f\"ROC AUC Score: {roc_auc:.2f}\")\n",
    "    \n",
    "    loss = log_loss(y_test, model.predict_proba(X_test))\n",
    "    print(f\"Log Loss: {loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, item in neural_networks.items():\n",
    "    pass  # Requires a more complex loop, will set up later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
